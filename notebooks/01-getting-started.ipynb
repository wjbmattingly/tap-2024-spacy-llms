{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e89c5b3",
   "metadata": {},
   "source": [
    "<img align=\"left\" src=\"https://ithaka-labs.s3.amazonaws.com/static-files/images/tdm/tdmdocs/tapi-logo-small.png\" />\n",
    "\n",
    "This notebook free for educational reuse under [Creative Commons CC BY License](https://creativecommons.org/licenses/by/4.0/).\n",
    "\n",
    "Created by [Firstname Lastname](https://) for the 2024 Text Analysis Pedagogy Institute, with support from [Constellate](https://constellate.org).\n",
    "\n",
    "For questions/comments/improvements, email author@email.address.<br />\n",
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f932d1",
   "metadata": {},
   "source": [
    "# `spaCy in the World of LLMs` `1`\n",
    "\n",
    "This is lesson `1` of 3 in the educational series on `spaCy and Large Language Models (LLMs)`. This notebook is intended `introduce students to spaCy, while also considering the current state of the field of natural language processing (NLP)`.\n",
    "\n",
    "**Skills:** \n",
    "* Python\n",
    "\n",
    "**Audience:** `Teachers` / `Learners` / `Researchers`\n",
    "\n",
    "**Use case:** `Tutorial` / `Reference` / `Explanation` \n",
    "\n",
    "`Include the use case definition from [here](https://constellate.org/docs/documentation-categories)`\n",
    "\n",
    "**Difficulty:** `Beginner`\n",
    "\n",
    "`Beginner assumes users are relatively new to Python and Jupyter Notebooks. The user is helped step-by-step with lots of explanatory text.`\n",
    "`Intermediate assumes users are familiar with Python and have been programming for 6+ months. Code makes up a larger part of the notebook and basic concepts related to Python are not explained.`\n",
    "`Advanced assumes users are very familiar with Python and have been programming for years, but they may not be familiar with the process being explained.`\n",
    "\n",
    "**Completion time:** `90 minutes`\n",
    "\n",
    "**Knowledge Required:** \n",
    "```\n",
    "* Python basics (variables, flow control, functions, lists, dictionaries)\n",
    "```\n",
    "\n",
    "**Knowledge Recommended:**\n",
    "```\n",
    "* A general understanding of natural language processing (NLP)\n",
    "```\n",
    "\n",
    "**Learning Objectives:**\n",
    "After this lesson, learners will be able to:\n",
    "```\n",
    "1. Understand the main concepts of natural language processing (NLP)\n",
    "2. Understand the role of spaCy within NLP\n",
    "3. Understand the key advantages and disadvantages of large language models.\n",
    "```\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157c0555",
   "metadata": {},
   "source": [
    "# Required Python Libraries\n",
    "`List out any libraries used and what they are used for`\n",
    "* [spaCy](https://spacy.io/) for performing [natural language processing](https://docs.constellate.org/key-terms/#nlp).\n",
    "\n",
    "## Install Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a220f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Install Libraries ###\n",
    "\n",
    "# Using !pip installs\n",
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c84fb074",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.7.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m60.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /Applications/anaconda3/envs/tap/lib/python3.10/site-packages (from en-core-web-sm==3.7.1) (3.7.5)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /Applications/anaconda3/envs/tap/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Applications/anaconda3/envs/tap/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Applications/anaconda3/envs/tap/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Applications/anaconda3/envs/tap/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Applications/anaconda3/envs/tap/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /Applications/anaconda3/envs/tap/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.5)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /Applications/anaconda3/envs/tap/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Applications/anaconda3/envs/tap/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Applications/anaconda3/envs/tap/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /Applications/anaconda3/envs/tap/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /Applications/anaconda3/envs/tap/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.12.3)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Applications/anaconda3/envs/tap/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Applications/anaconda3/envs/tap/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /Applications/anaconda3/envs/tap/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.8.2)\n",
      "Requirement already satisfied: jinja2 in /Applications/anaconda3/envs/tap/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.4)\n",
      "Requirement already satisfied: setuptools in /Applications/anaconda3/envs/tap/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (69.5.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /Applications/anaconda3/envs/tap/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (24.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Applications/anaconda3/envs/tap/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /Applications/anaconda3/envs/tap/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.4)\n",
      "Requirement already satisfied: language-data>=1.2 in /Applications/anaconda3/envs/tap/lib/python3.10/site-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Applications/anaconda3/envs/tap/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in /Applications/anaconda3/envs/tap/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.20.1)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /Applications/anaconda3/envs/tap/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Applications/anaconda3/envs/tap/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Applications/anaconda3/envs/tap/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Applications/anaconda3/envs/tap/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Applications/anaconda3/envs/tap/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2024.7.4)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /Applications/anaconda3/envs/tap/lib/python3.10/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Applications/anaconda3/envs/tap/lib/python3.10/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.5)\n",
      "Requirement already satisfied: click>=8.0.0 in /Applications/anaconda3/envs/tap/lib/python3.10/site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /Applications/anaconda3/envs/tap/lib/python3.10/site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /Applications/anaconda3/envs/tap/lib/python3.10/site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (13.7.1)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /Applications/anaconda3/envs/tap/lib/python3.10/site-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.18.1)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /Applications/anaconda3/envs/tap/lib/python3.10/site-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (7.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Applications/anaconda3/envs/tap/lib/python3.10/site-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.5)\n",
      "Requirement already satisfied: marisa-trie>=0.7.7 in /Applications/anaconda3/envs/tap/lib/python3.10/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Applications/anaconda3/envs/tap/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Applications/anaconda3/envs/tap/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.18.0)\n",
      "Requirement already satisfied: wrapt in /Applications/anaconda3/envs/tap/lib/python3.10/site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.16.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Applications/anaconda3/envs/tap/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.2)\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.7.1\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5480e2a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Import Libraries ###\n",
    "import spacy\n",
    "from spacy import displacy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53edaa2",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "```\n",
    "Introduce the lesson topic. Answer questions such as:\n",
    "* Why is it useful? \n",
    "* Why should we learn it? \n",
    "* Who might use it? \n",
    "* Where has it been used by scholars/industry?\n",
    "* What do we need to do it?\n",
    "* What subjects are included in the notebooks?\n",
    "* What is not in this notebook? Where should we look for it?\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c53fe71b",
   "metadata": {},
   "source": [
    "# What is Natural Language Processing (NLP)\n",
    "\n",
    "Named entity recognition (addressed below) is a branch of natural language processing, better known as NLP. NLP is the process by which a researcher uses a computer system to parse human language and extract important metadata from texts. The purpose of NLP is to perform, among other things, distant reading.\n",
    "\n",
    "Distant reading has a long history extending to the late-twentieth century. It is commonly used when the quantity of texts in a given corpus prevent a researcher (or a team of researchers) from reading the corpus closely in its entirety. In order to make sense of that large corpus, the researcher will often pass certain tasks to a computer with the understanding that there is a margin of error. This margin of error is accepted in exchange for the ability to gain a larger, distant understanding of that corpus. Distant reading is used to perform several significant tasks, such as:\n",
    "\n",
    "1. sentiment analysis=> understanding the sentiment of a text\n",
    "2. text classification=> classify texts into predetermined categories\n",
    "3. named entity recognition=> extract entities from a text\n",
    "\n",
    "The metadata from these tasks can then be used to get a sense of the texts without reading them closely, hence the term distant reading.\n",
    "\n",
    "\n",
    "# What are Frameworks?\n",
    "\n",
    "In order to engage in NLP, a researcher must first decide upon the framework they wish to use. Framework is a word that describes the software used by the researcher to engage in a specific task. A good way to think about a framework in Pythonic terms is as a library, or packaged set of usable classes and functions to perform complex tasks easily. Deciding which framework to use depends on a few variables. I will use the word “Pythonic” throughout this book. Pythonic is a term programmers of Python use to refer to the standard, or community-accepted, way to do something. A good example is the way in which one imports pandas, a library for analyzing and working with tabular data. When we import pandas, we import it as pd. Why? Because the documentation told us to do so and, perhaps even more importantly, everyone in the community follows this syntax.\n",
    "\n",
    "First, not all frameworks support all languages and not all frameworks support the same languages equally.\n",
    "\n",
    "Second, certain frameworks perform certain tasks better than others. While all frameworks will tokenize equally well (usually), the way in which some tasks, such as finding the root of words via lemmatization (spaCy) vs. stemming (Stanza) will vary. Decision on a framework for this purpose typically lies in the realm of computational linguistics or distance reading for the purpose of finding how a word (or words) appear in texts in all forms (conjugated and declined).\n",
    "\n",
    "A common third thing to consider is the way in which the framework performs NLP. There are essentially two methods for performing NLP: rules-based and machine learning-based. Rules-based NLP is the process by which the frameworks has a predetermined set of rules for how to handle specific tasks. In order to find entities in a text, for example, a rules-based method will contain a dictionary of all types of entities or it may contain a RegEx formula for identifying patterns that match an entity.\n",
    "\n",
    "Most frameworks today are moving away from a rules-based approach to NLP in favor of a machine learning-based approach. Machine learning-based NLP is the process by which developers use statistics to teach a computer system (known as a model) to perform a task based on past experiences (known as training). We will be speaking much more about machine learning-based NLP later in a later notebook as spaCy, the chief subject of this notebook, is a machine learning-based Python library.\n",
    "\n",
    "# What is spaCy?\n",
    "\n",
    "The spaCy (spelled correctly) library is a robust machine learning NLP library developed by Explosion AI, a Berlin based team of computer scientists and computational linguists. It supports a wide variety of European languages out-of-the-box with statistical models capable of parsing texts, identifying parts-of-speech, and extract entities. SpaCy is also capable of easily improving or training from scratch custom models on domain-specific texts.\n",
    "\n",
    "In this notebook, we will go through the steps for installing spaCy, downloading a pretrained language model, and performing the essential tasks of NLP."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72fc3469",
   "metadata": {},
   "source": [
    "## How to Install spaCy\n",
    "\n",
    "In order to install spaCy, I recommend visiting their website, [here](https://spacy.io/usage).\n",
    "\n",
    "\n",
    "![find models](../images/spacy_demo.gif)\n",
    "\n",
    "They have a nice user-friendly interface. Input your device settings, e.g. Mac or Windows or Linux, and your language, e.g. English, French, or German. The web-app will automatically populate the commands that you need to execute to get started. Since this is a JupyterBook, we can install these with a “!” in a cell to indicate that we want to run a terminal command. I will be installing spaCy and thee small English model, en_core_web_sm.\n",
    "\n",
    "```bash\n",
    "!pip install spacy\n",
    "```\n",
    "```bash\n",
    "!python -m spacy download en_core_web_sm\n",
    "```\n",
    "\n",
    "Now that we’ve installed spaCy let’s import it to make sure we installed it correctly. (These steps were done above to conform to the TAP Institute's Format)\n",
    "\n",
    "```python\n",
    "import spacy\n",
    "```\n",
    "\n",
    "Great! Now, let’s make sure we downloaded the model successfully with the command below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "44b9d247",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c31d9d5",
   "metadata": {},
   "source": [
    "Excellent! spaCy is now installed correctly and we have successfully downloaded the small English model. We will pick up here with the code in the next notebook. For now, I want to focus on big-picture items, specifically spaCy “containers”.\n",
    "\n",
    "# Containers\n",
    "\n",
    "Containers are spaCy objects that contain a large quantity of data about a text. When we analyze texts with the spaCy framework, we create different container objects to do that. Here is a full list of all spaCy containers. We will be focusing on three (emboldened): Doc, Span, and Token.\n",
    "\n",
    "1. **Doc**\n",
    "2. DocBin\n",
    "3. Example\n",
    "4. Language\n",
    "5. Lexeme\n",
    "6. **Span**\n",
    "7. SpanGroup\n",
    "8. **Token**\n",
    "\n",
    "I created the image below to show how I visualize spaCy containers in my mind. At the top, we have a Doc container. This is the basis for all spaCy. It is the main object that we create. Within the Doc container are many different attributes and subcontainers. One attribute is the Doc.sents, which contains all the sentences in the Doc container. The doc container (and each sentence generator) is made up of a set of token containers. These are things like words, punctuation, etc.\n",
    "\n",
    "Span containers are kind of like a token, in that they are a piece of a Doc container. Spans have one thing that makes them unique. They can cross multiple tokens.\n",
    "\n",
    "We can give spans a bit more specificity by classifying them into different groups. These are known as SpanGroup containers.\n",
    "\n",
    "![pyramid](https://python-textbook.pythonhumanities.com/_images/spacy_containers.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce087f0",
   "metadata": {},
   "source": [
    "# The spaCy `Doc`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af710892",
   "metadata": {},
   "source": [
    "With spaCy imported, we can now create our nlp object. This is the standard Pythonic way to create your model in a Python script. Unless you are working with multiple models in a script, try to always name your model, nlp. It will make your script much easier to read. To do this, we will use spacy.load(). This command tells spaCy to load up a model. In order to know which model to load, it needs a string argument that corresponds to the model name. Since we will be working with the small English model, we will use “en_core_web_sm”. This function can take keyword arguments to identify which parts of the model you want to load, but we will get to that later. For now, we want to import the whole thing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "f1c88494",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fceac99",
   "metadata": {},
   "source": [
    "Great! With the model loaded, let’s go ahead and create a text. This text comes from the Founders Online Archive (see citation below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b0e66270",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From George Washington to John Adam, 15 October 1780,” Founders Online, National Archives, https://founders.archives.gov/documents/Washington/03-28-02-0299. [Original source: The Papers of George Washington, Revolutionary War Series, vol. 28, 28 August–27 October 1780, ed. William M. Ferraro and Jeffrey L. Zvengrowski. Charlottesville: University of Virginia Press, 2020, p. 558.\n",
    "\n",
    "text = \"\"\"I understand Mr Skinner is gone to Philadelphia. You will keep the inclosed letter for him till he returns, when You will take the earliest opportunity of delivering it to him. I desire to see him as soon as he arrives & have written to him for the purpose.\n",
    "You will inform the Officer who came with a flag to Elizabeth Town Yesterday—that he is not to wait for an answer to the letters he brought; and that One will be transmitted by an early conveyance. You will deliver him the letters in the packet which accompanies this.\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e126e24b",
   "metadata": {},
   "source": [
    "With the data loaded in, it’s time to make our first Doc container. Unless you are working with multiple Doc containers, it is best practice to always call this object “doc”, all lowercase. To create a doc container, we will usually just call our nlp object and pass our text to it as a single argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "37f4d3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bbef4cb",
   "metadata": {},
   "source": [
    "Great! Let’s see what this looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "a409796d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "I understand Mr Skinner is gone to Philadelphia. You will keep the inclosed letter for him till he returns, when You will take the earliest opportunity of delivering it to him. I desire to see him as soon as he arrives & have written to him for the purpose.\n",
       "You will inform the Officer who came with a flag to Elizabeth Town Yesterday—that he is not to wait for an answer to the letters he brought; and that One will be transmitted by an early conveyance. You will deliver him the letters in the packet which accompanies this."
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a18678",
   "metadata": {},
   "source": [
    "Let's take a look at this `doc` a bit more closely. Let's count its length with the `len()` function! Remember, the `len()` function lets you count the length of an object in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "67e094ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "108"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd2b8c2",
   "metadata": {},
   "source": [
    "Why is this length so low? There are clearly more than 108 characters in the text above! The reason? This is counting the tokens in the `Doc` container. Let's iterate over the first ten items in the `Doc` and take a closer look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ea0b0c92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I\n",
      "understand\n",
      "Mr\n",
      "Skinner\n",
      "is\n",
      "gone\n",
      "to\n",
      "Philadelphia\n",
      ".\n",
      "You\n"
     ]
    }
   ],
   "source": [
    "for token in doc[:10]:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7ab3e7",
   "metadata": {},
   "source": [
    "## Tokens\n",
    "\n",
    "While on the surface it may seem that the Doc container’s length is dependent on the quantity of words, look more closely. You should notice that the `.` is also considered an item in the container. These are all known as tokens. Tokens are a fundamental building block of spaCy or any NLP framework. They can be words or punctuation marks. Tokens are something that has syntactic purpose in a sentence and is self-contained. A good example of this is the contraction “don’t” in English. When tokenized, or the process of converting the text into tokens, we will have two tokens. “do” and “n’t” because the contraction represents two words, “do” and “not”.\n",
    "\n",
    "Let's learn a bit more about tokens by examining our first token more closely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "91da94a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_token = doc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9108b0",
   "metadata": {},
   "source": [
    "Let's now examine `first_token` to see the different attributes associated with it. We can use the `dir()` function to do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "6015de24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'_, __bytes__, __class__, __delattr__, __dir__, __doc__, __eq__, __format__, __ge__, __getattribute__, __gt__, __hash__, __init__, __init_subclass__, __le__, __len__, __lt__, __ne__, __new__, __pyx_vtable__, __reduce__, __reduce_ex__, __repr__, __setattr__, __sizeof__, __str__, __subclasshook__, __unicode__, ancestors, check_flag, children, cluster, conjuncts, dep, dep_, doc, ent_id, ent_id_, ent_iob, ent_iob_, ent_kb_id, ent_kb_id_, ent_type, ent_type_, get_extension, has_dep, has_extension, has_head, has_morph, has_vector, head, i, idx, iob_strings, is_alpha, is_ancestor, is_ascii, is_bracket, is_currency, is_digit, is_left_punct, is_lower, is_oov, is_punct, is_quote, is_right_punct, is_sent_end, is_sent_start, is_space, is_stop, is_title, is_upper, lang, lang_, left_edge, lefts, lemma, lemma_, lex, lex_id, like_email, like_num, like_url, lower, lower_, morph, n_lefts, n_rights, nbor, norm, norm_, orth, orth_, pos, pos_, prefix, prefix_, prob, rank, remove_extension, right_edge, rights, sent, sent_start, sentiment, set_extension, set_morph, shape, shape_, similarity, subtree, suffix, suffix_, tag, tag_, tensor, text, text_with_ws, vector, vector_norm, vocab, whitespace_'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\", \".join(dir(first_token))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43789452",
   "metadata": {},
   "source": [
    "As we can see there are quite a bit of attributes attached to each token. This is one of the large strengths of using an NLP framework like spaCy to process documents. Let's take a closer look at some of the more important ones in the table below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e348ef3",
   "metadata": {},
   "source": [
    "### Table for spaCy Token Attributes\n",
    "\n",
    "| Name        | Description                                  | Code Example          |\n",
    "|-------------|----------------------------------------------|-----------------------|\n",
    "| `sent`      | The sentence to which the token belongs.     | `token.sent`          |\n",
    "| `text`      | The raw text of the token.                   | `token.text`          |\n",
    "| `head`      | The parent of the token in the dependency tree. | `token.head`         |\n",
    "| `left_edge` | The leftmost token of the token's subtree.   | `token.left_edge`     |\n",
    "| `right_edge`| The rightmost token of the token's subtree.  | `token.right_edge`    |\n",
    "| `ent_type_` | The entity type label of the token, if any.  | `token.ent_type_`     |\n",
    "| `lemma_`    | The lemmatized form of the token.            | `token.lemma_`        |\n",
    "| `morph`     | The morphological details of the token.      | `token.morph`         |\n",
    "| `pos_`      | The part of speech tag of the token.         | `token.pos_`          |\n",
    "| `dep_`      | The syntactic dependency relation.           | `token.dep_`          |\n",
    "| `lang_`     | The language of the parent document.         | `token.lang_`         |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473b55a0",
   "metadata": {},
   "source": [
    "The most important of these will be `pos_`, `dep_`, and `lemma_`. These allow you to do a significant amount of linguistic analysis on a document. Let's view each of these now in code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "1864a61f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'PRON'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_token.pos_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "be82079e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'nsubj'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_token.dep_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "19d61fdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_token.lemma_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bcb45cc",
   "metadata": {},
   "source": [
    "### Visualizing Token Parts of Speech\n",
    "\n",
    "Often when we process documents in bulk, we don't closely examine each document. However, sometimes it is necessary to examine documents closely to see how the model is performing or to get a deeper sense of a small sample of our data. While you could iterate through each token and look at the outputs, it is usually helpful to visualize the parts of speech of each token collectively.\n",
    "\n",
    "We can do this with `displacy()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "b30612ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"8b08c70bc7404b02abc84ee34d36fded-0\" class=\"displacy\" width=\"1250\" height=\"362.0\" direction=\"ltr\" style=\"max-width: none; height: 362.0px; color: white; background: #09a3d5; font-family: Source Sans Pro; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"272.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">I</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PRON</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"272.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"200\">understand</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"200\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"272.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"350\">Mr</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"350\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"272.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"500\">Skinner</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"500\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"272.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"650\">is</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"650\">AUX</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"272.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"800\">gone</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"800\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"272.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"950\">to</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"950\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"272.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1100\">Philadelphia.</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1100\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-8b08c70bc7404b02abc84ee34d36fded-0-0\" stroke-width=\"2px\" d=\"M62,227.0 62,202.0 194.0,202.0 194.0,227.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-8b08c70bc7404b02abc84ee34d36fded-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M62,229.0 L58,221.0 66,221.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-8b08c70bc7404b02abc84ee34d36fded-0-1\" stroke-width=\"2px\" d=\"M362,227.0 362,202.0 494.0,202.0 494.0,227.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-8b08c70bc7404b02abc84ee34d36fded-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M362,229.0 L358,221.0 366,221.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-8b08c70bc7404b02abc84ee34d36fded-0-2\" stroke-width=\"2px\" d=\"M512,227.0 512,177.0 797.0,177.0 797.0,227.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-8b08c70bc7404b02abc84ee34d36fded-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubjpass</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M512,229.0 L508,221.0 516,221.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-8b08c70bc7404b02abc84ee34d36fded-0-3\" stroke-width=\"2px\" d=\"M662,227.0 662,202.0 794.0,202.0 794.0,227.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-8b08c70bc7404b02abc84ee34d36fded-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">auxpass</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M662,229.0 L658,221.0 666,221.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-8b08c70bc7404b02abc84ee34d36fded-0-4\" stroke-width=\"2px\" d=\"M212,227.0 212,152.0 800.0,152.0 800.0,227.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-8b08c70bc7404b02abc84ee34d36fded-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">ccomp</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M800.0,229.0 L804.0,221.0 796.0,221.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-8b08c70bc7404b02abc84ee34d36fded-0-5\" stroke-width=\"2px\" d=\"M812,227.0 812,202.0 944.0,202.0 944.0,227.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-8b08c70bc7404b02abc84ee34d36fded-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M944.0,229.0 L948.0,221.0 940.0,221.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-8b08c70bc7404b02abc84ee34d36fded-0-6\" stroke-width=\"2px\" d=\"M962,227.0 962,202.0 1094.0,202.0 1094.0,227.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-8b08c70bc7404b02abc84ee34d36fded-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1094.0,229.0 L1098.0,221.0 1090.0,221.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "options = {\"compact\": True, \"bg\": \"#09a3d5\",\n",
    "           \"color\": \"white\", \"font\": \"Source Sans Pro\"}\n",
    "first_sentence = list(doc.sents)[0]\n",
    "displacy.render(first_sentence, style=\"dep\", options=options)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bcd130a",
   "metadata": {},
   "source": [
    "## Sentence Boundary Detection (SBD)\n",
    "\n",
    "In NLP, sentence boundary detection, or SBD, is the identification of sentences in a text. Again, this may seem fairly easy to do with rules. One could use split(“.”), but in English we use the period to also denote abbreviation. You could, again, write rules to look for periods not proceeded by a lowercase word, but again, I ask the question, “why bother?”. We can use spaCy and in seconds have all sentences fully separated through SBD.\n",
    "\n",
    "To access the sentences in the Doc container, we can use the attribute sents, like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "4d26b578",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator at 0x3190fb420>"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.sents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a90d0075",
   "metadata": {},
   "source": [
    "Uh oh! This may not be the expected output. This is known as a `generator`, a more advanced aspect of Python. Think of a generator as an efficient way to store data in memory that can be accessed iteratively, rather than loaded in memory all at once. Why does this matter? It matters for two reasons. First, it matters because it means that you can load larger documents more efficiently on your system via spaCy.\n",
    "\n",
    "Secondly, it matters because it affects how you access that data. You cannot index a generator like you can a list. For example, were I to try and access the first sentence (index 0), I cannot do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "0c20115c",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'generator' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/wjbmattingly/tap-2024-spacy-llms/notebooks/01-getting-started.ipynb Cell 41\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/wjbmattingly/tap-2024-spacy-llms/notebooks/01-getting-started.ipynb#Y111sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m doc\u001b[39m.\u001b[39;49msents[\u001b[39m0\u001b[39;49m]\n",
      "\u001b[0;31mTypeError\u001b[0m: 'generator' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "doc.sents[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d2d246",
   "metadata": {},
   "source": [
    "How then do you access the data? There are a couple options at your disposal. First, if you need to iterate over each sentence, you can iterate over the data precisely the same way you would if it were a list. In the example below, we are using a `for` loop to iterate over the generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "c50c1788",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I understand Mr Skinner is gone to Philadelphia.\n",
      "You will keep the inclosed letter for him till he returns, when You will take the earliest opportunity of delivering it to him.\n",
      "I desire to see him as soon as he arrives & have written to him for the purpose.\n",
      "\n",
      "You will inform the Officer who came with a flag to Elizabeth Town Yesterday—that he is not to wait for an answer to the letters he brought; and that One will be transmitted by an early conveyance.\n",
      "You will deliver him the letters in the packet which accompanies this.\n"
     ]
    }
   ],
   "source": [
    "for sent in doc.sents:\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b713d6",
   "metadata": {},
   "source": [
    "But what if we wanted to access a specific index of the sentences. To do this, we need to convert the `doc.sents` `generator` into a list. We can do this with the `list()` function. **WARNING!** If you do this, remember, you will load the entire generator into memory. If you are working with a single document, this likely will not be an issue, but if you are working with large quantities of data this can cause an issue with memory and cause your Python to return and `Out of Memory` error and crash.\n",
    "\n",
    "To do this, we can use the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "08bdd425",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = list(doc.sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49222a58",
   "metadata": {},
   "source": [
    "Now, I can access index 0 of sentences without issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "35e46eea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "I understand Mr Skinner is gone to Philadelphia."
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13f16ea",
   "metadata": {},
   "source": [
    "SpaCy does far more than simply do sentence boundary detection, though. It also provides us the same metadata for each sentence that we have at the `Doc` level. This is because a sentence is technically a `Span` in spaCy terms. This means it functions underneath the umbrella of `Doc`, but functions conceptually as a similar item. This means that we can iterate over the tokens of a sentence, just like as we would if it were a `Doc`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "391a98a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I\n",
      "understand\n",
      "Mr\n",
      "Skinner\n",
      "is\n",
      "gone\n",
      "to\n",
      "Philadelphia\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "for sentence_token in sentences[0]:\n",
    "    print(sentence_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9cba1a1",
   "metadata": {},
   "source": [
    "And again, the tokens themselves each have the same precise metadata. We can, for example, grab each token's part of speech with the `.pos_` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "895d045c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I PRON\n",
      "understand VERB\n",
      "Mr PROPN\n",
      "Skinner PROPN\n",
      "is AUX\n",
      "gone VERB\n",
      "to ADP\n",
      "Philadelphia PROPN\n",
      ". PUNCT\n"
     ]
    }
   ],
   "source": [
    "for sentence_token in sentences[0]:\n",
    "    print(sentence_token, sentence_token.pos_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6421d01e",
   "metadata": {},
   "source": [
    "## Named Entity Recognition\n",
    "\n",
    "`Entities` are words in a text that correspond to a specific type of data. They can be numerical, such as cardinal numbers; temporal, such as dates; nominal, such as names of people and places; and political, such as geopolitical entities (GPE). In short, an entity can be anything the designer wishes to designate as an item in a text that has a corresponding label.\n",
    "\n",
    "Named entity recognition, or NER, is the process by which a system takes an input of unstructured data (a text) and outputs structured data, specifically the identification of entities. Let us consider this short example.\n",
    "\n",
    "Martha, a senior, moved to Spain where she will be playing basketball until 05 June 2022 or until she can’t play any longer.\n",
    "\n",
    "In this example, we have several potential entities. First, there is “Martha”. Different NER models will have different corresponding labels for such an entity, but PERSON or PER is considered standard practice. Note here that the label is capitalized. This is also standard practice. We also have a GPE, or Geopolitical Entity, notably “Spain”. Finally, we have a DATE entity, “05 June 2022”. These are standard labels that one can expect to extract from a text. If the domain at hand, however, has additional labels, those can be extracted as well. Perhaps the client or user wants to not only extract people, GPEs, and dates, but also sports. In such a scenario “basketball” could be extracted and given the label SPORT.\n",
    "\n",
    "Not all entities are singular. As is common with texts, sometimes entities are `Multi-word Span`. Let us consider the same sentence as above, but with one modification:\n",
    "\n",
    "Martha Thompson, a senior, moved to Spain where she will be playing basketball until 05 June 2022 or until she can’t play any longer.\n",
    "\n",
    "Here, Martha now has a surname, “Thompson”. We can either extract Martha and Thompson as individual entities or, as is more common practice, extract both as a single entity, since “Martha Thompson” is a single individual. An NER system, therefore, should recognize “Martha Thompson” as a single, `Span`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "cf869608",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">I understand Mr \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Skinner\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " is gone to \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Philadelphia\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       ". You will keep the inclosed letter for him till he returns, when You will take the earliest opportunity of delivering it to him. I desire to see him as soon as he arrives &amp; have written to him for the purpose.<br>You will inform the Officer who came with a flag to \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Elizabeth Town\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Yesterday\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       "—that he is not to wait for an answer to the letters he brought; and that One will be transmitted by an early conveyance. You will deliver him the letters in the packet which accompanies this.</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "displacy.render(doc, style=\"ent\", options=options)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "734dff0a",
   "metadata": {},
   "source": [
    "# Why use spaCy when we have ChatGPT?\n",
    "\n",
    "Now that we have a good sense about what spaCy can do, you may be asking yourself, how does this precisely fit into the world of LLMs. If ChatGPT can give me this same data, why would I even bother using spaCy? Over the next two lessons, we will answer this question."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca42822",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "\n",
    "Practice making spaCy Doc containers with different documents. Experiment with different model sizes. Explore where the models are good and, more importantly, where they are bad."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2368107",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c7fd8c",
   "metadata": {},
   "source": [
    "1. From George Washington to John Adam, 15 October 1780,” Founders Online, National Archives, https://founders.archives.gov/documents/Washington/03-28-02-0299. [Original source: The Papers of George Washington, Revolutionary War Series, vol. 28, 28 August–27 October 1780, ed. William M. Ferraro and Jeffrey L. Zvengrowski. Charlottesville: University of Virginia Press, 2020, p. 558."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
